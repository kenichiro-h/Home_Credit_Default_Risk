{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['application_test.csv', 'POS_CASH_balance.csv', 'credit_card_balance.csv', 'installments_payments.csv', 'application_train.csv', 'bureau.csv', 'previous_application.csv', 'bureau_balance.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "#for save & load pickle\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_as_pickled_object(obj, filepath):\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    n_bytes = sys.getsizeof(bytes_out)\n",
    "    with open(filepath, 'wb') as f_out:\n",
    "        for idx in range(0, n_bytes, max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "\n",
    "\n",
    "def try_to_load_as_pickled_object_or_None(filepath):\n",
    "    max_bytes = 2**31 - 1\n",
    "    try:\n",
    "        input_size = os.path.getsize(filepath)\n",
    "        bytes_in = bytearray(0)\n",
    "        with open(filepath, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        obj = pickle.loads(bytes_in)\n",
    "    except:\n",
    "        return None\n",
    "    return obj\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "PATH = \"../csv/\"\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def import_data(file):\n",
    "    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
    "    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 59.54 MB\n",
      "Decreased by 79.2%\n",
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 9.40 MB\n",
      "Decreased by 79.1%\n"
     ]
    }
   ],
   "source": [
    "#application_data\n",
    "application_train = import_data(PATH+'application_train.csv')\n",
    "application_test = import_data(PATH+'application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = application_train[application_train['AMT_INCOME_TOTAL'] != 1.170000e+08]\n",
    "application_train = application_train[application_train['AMT_REQ_CREDIT_BUREAU_QRT'] != 261]\n",
    "application_train = application_train[application_train['OBS_30_CNT_SOCIAL_CIRCLE'] < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train['DAYS_EMPLOYED'] = (application_train['DAYS_EMPLOYED'].apply(lambda x: x if x != 365243 else np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_ext_source(df):\n",
    "    x1 = df['EXT_SOURCE_1'].fillna(-1) + 1e-1\n",
    "    x2 = df['EXT_SOURCE_2'].fillna(-1) + 1e-1\n",
    "    x3 = df['EXT_SOURCE_3'].fillna(-1) + 1e-1\n",
    "    \n",
    "    df['EXT_SOURCE_1over2_NAminus1_Add0.1'] = x1/x2\n",
    "    df['EXT_SOURCE_2over1_NAminus1_Add0.1'] = x2/x1\n",
    "    df['EXT_SOURCE_1over3_NAminus1_Add0.1'] = x1/x3\n",
    "    df['EXT_SOURCE_3over1_NAminus1_Add0.1'] = x3/x1\n",
    "    df['EXT_SOURCE_2over3_NAminus1_Add0.1'] = x2/x3\n",
    "    df['EXT_SOURCE_3over2_NAminus1_Add0.1'] = x3/x2\n",
    "    \n",
    "    df['EXT_SOURCE_na1_2'] = (application_train['EXT_SOURCE_1'].isnull()) * (application_train['EXT_SOURCE_2'].fillna(0))\n",
    "    df['EXT_SOURCE_na1_3'] = (application_train['EXT_SOURCE_1'].isnull()) * (application_train['EXT_SOURCE_3'].fillna(0))\n",
    "    df['EXT_SOURCE_na2_1'] = (application_train['EXT_SOURCE_2'].isnull()) * (application_train['EXT_SOURCE_1'].fillna(0))\n",
    "    df['EXT_SOURCE_na2_3'] = (application_train['EXT_SOURCE_2'].isnull()) * (application_train['EXT_SOURCE_3'].fillna(0))\n",
    "    df['EXT_SOURCE_na3_1'] = (application_train['EXT_SOURCE_3'].isnull()) * (application_train['EXT_SOURCE_1'].fillna(0))\n",
    "    df['EXT_SOURCE_na3_2'] = (application_train['EXT_SOURCE_3'].isnull()) * (application_train['EXT_SOURCE_2'].fillna(0))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = feat_ext_source(application_train)\n",
    "application_test  = feat_ext_source(application_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of train increases from 79.86 to 115.22 MB\n",
      "Memory usage of test increases from 10.51 to 16.13 MB\n"
     ]
    }
   ],
   "source": [
    "# use this if you want to convert categorical features to dummies(default)\n",
    "def cat_to_dummy(train, test):\n",
    "    train_d = pd.get_dummies(train, drop_first=False)\n",
    "    test_d = pd.get_dummies(test, drop_first=False)\n",
    "    # make sure that the number of features in train and test should be same\n",
    "    for i in train_d.columns:\n",
    "        if i not in test_d.columns:\n",
    "            if i!='TARGET':\n",
    "                train_d = train_d.drop(i, axis=1)\n",
    "    for j in test_d.columns:\n",
    "        if j not in train_d.columns:\n",
    "            if j!='TARGET':\n",
    "                test_d = test_d.drop(i, axis=1)\n",
    "    print('Memory usage of train increases from {:.2f} to {:.2f} MB'.format(train.memory_usage().sum() / 1024**2, \n",
    "                                                                            train_d.memory_usage().sum() / 1024**2))\n",
    "    print('Memory usage of test increases from {:.2f} to {:.2f} MB'.format(test.memory_usage().sum() / 1024**2, \n",
    "                                                                            test_d.memory_usage().sum() / 1024**2))\n",
    "    return train_d, test_d\n",
    "\n",
    "application_train_ohe, application_test_ohe = cat_to_dummy(application_train, application_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_data = try_to_load_as_pickled_object_or_None('/Volumes/sub/kaggle/pkl/Home Credit Default Risk/bureau_data.pkl')\n",
    "previous_data = try_to_load_as_pickled_object_or_None('/Volumes/sub/kaggle/pkl/Home Credit Default Risk/previous_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306487\n",
      "306487\n",
      "48744\n",
      "48744\n"
     ]
    }
   ],
   "source": [
    "print(len(application_train_ohe))\n",
    "application_train_ohe2 = pd.merge(application_train_ohe, previous_data, on='SK_ID_CURR',how='left')\n",
    "application_train_ohe2 = pd.merge(application_train_ohe2, bureau_data, on='SK_ID_CURR',how='left')\n",
    "#application_train_ohe2 = application_train_ohe2.fillna(0)\n",
    "print(len(application_train_ohe2))\n",
    "\n",
    "print(len(application_test_ohe))\n",
    "application_test_ohe2 = pd.merge(application_test_ohe, previous_data, on='SK_ID_CURR',how='left')\n",
    "application_test_ohe2 = pd.merge(application_test_ohe2, bureau_data, on='SK_ID_CURR',how='left')\n",
    "#application_test_ohe2 = application_test_ohe2.fillna(0)\n",
    "print(len(application_test_ohe2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_pickled_object(application_train_ohe2,'/Volumes/sub/kaggle/pkl/Home Credit Default Risk/train_data.pkl')\n",
    "save_as_pickled_object(application_test_ohe2,'/Volumes/sub/kaggle/pkl/Home Credit Default Risk/test_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
